{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thong/.virtualenvs/minerva/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import Enum\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change if on kaggle\n",
    "KAGGLE_PATH = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSet(Enum):\n",
    "    full = 1\n",
    "    resized = 2\n",
    "    cropped = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(22)\n",
    "def rotate_lucky_img():\n",
    "    lucky_num = random.randint(1, 10)\n",
    "    if lucky_num < 0.2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_RESIZE = (256, 256)\n",
    "CENTER_CROP = (224, 224)\n",
    "INCEPTIONV3_RESIZE = (299, 299) # InceptionNetV3 requires image sizes of 299x299\n",
    "\n",
    "def test_image_transform(image_set = ImageSet.full):\n",
    "    if ImageSet.full == image_set:\n",
    "        img_transforms = v2.Compose([\n",
    "            v2.Resize(IMAGE_RESIZE),\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "    else:\n",
    "        img_transforms = v2.Compose([\n",
    "            v2.ToDtype(torch.float32, scale=True)\n",
    "        ])\n",
    "\n",
    "    return img_transforms\n",
    "\n",
    "def crop_image_transform():\n",
    "    img_transforms = v2.Compose([\n",
    "        v2.Resize(IMAGE_RESIZE),\n",
    "        # v2.CenterCrop(CENTER_CROP),\n",
    "        v2.RandomHorizontalFlip(0.2),\n",
    "        v2.RandomVerticalFlip(0.2),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    return img_transforms\n",
    "\n",
    "def random_rotate_transform():\n",
    "    img_transforms = v2.Compose([\n",
    "        v2.RandomRotation(0, 180),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    return img_transforms\n",
    "\n",
    "def train_image_transform():\n",
    "    img_transforms = v2.Compose([\n",
    "        v2.RandomHorizontalFlip(0.2),\n",
    "        v2.RandomVerticalFlip(0.2),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    return img_transforms\n",
    "\n",
    "def inception_image_transform():\n",
    "    img_transforms = v2.Compose([\n",
    "        v2.Resize(INCEPTIONV3_RESIZE),\n",
    "        v2.RandomHorizontalFlip(0.2),\n",
    "        v2.RandomVerticalFlip(0.2),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    return img_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_dataset(image_paths, image_set=ImageSet.resized, train_inception=False, run_test_image=False):\n",
    "    images = []\n",
    "\n",
    "    for image_path in tqdm(image_paths):\n",
    "        # for resized images, can read in using torchvision\n",
    "        # for full sized images, need to use PIL as not all images are jpeg format\n",
    "        if ImageSet.resized == image_set:\n",
    "            image_tensor = read_image(image_path, mode=ImageReadMode.RGB)\n",
    "        else:\n",
    "            with Image.open(image_path) as pil_image:\n",
    "                image_tensor = TF.to_tensor(pil_image.convert('RGB'))\n",
    "\n",
    "        # setting transforms, InceptionNet V3 requires the size of 299x299\n",
    "        # therefore it gets its own transformer\n",
    "        if run_test_image:\n",
    "            img_transformer = test_image_transform(image_set)\n",
    "        elif train_inception:\n",
    "            img_transformer = inception_image_transform()\n",
    "        elif ImageSet.full == image_set:\n",
    "            img_transformer = crop_image_transform()\n",
    "        else:\n",
    "            img_transformer = train_image_transform()\n",
    "\n",
    "        image_tensor = img_transformer(image_tensor)\n",
    "        images.append(image_tensor)\n",
    "\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the load_dataset from the starter notebook as a template.\n",
    "# converting this function to work with pytorch\n",
    "def load_dataset_pil(image_paths, is_training=False, use_resized=True):\n",
    "    images = []\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        if use_resized:\n",
    "            pil_image = Image.open(image_path)\n",
    "        else:\n",
    "            pil_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if is_training:\n",
    "            if rotate_lucky_img():\n",
    "                img_transform = random_rotate_transform()\n",
    "            else:\n",
    "                img_transform = train_image_transform()\n",
    "            pil_image = img_transform(pil_image)\n",
    "        else:\n",
    "            pil_image = TF.to_tensor(pil_image)\n",
    "\n",
    "        numpy_image = np.asarray(pil_image)\n",
    "        images.append(numpy_image)\n",
    "\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensor_datasets(x_train, x_val, y_train, y_val):\n",
    "\n",
    "    x_train_tensor = torch.Tensor(x_train)\n",
    "    x_val_tensor = torch.Tensor(x_val)\n",
    "\n",
    "    y_train_tensor = torch.Tensor(y_train)\n",
    "    y_val_tensor = torch.Tensor(y_val)\n",
    "\n",
    "    # Resized Training Images\n",
    "    ds_train_images = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    ds_val_images = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "    return ds_train_images, ds_val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_dataloader(test_images, batch_size):\n",
    "    test_image_tensor = torch.Tensor(test_images)\n",
    "    ds_test_images = TensorDataset(test_image_tensor)\n",
    "\n",
    "    test_dl = DataLoader(ds_test_images, batch_size=batch_size)\n",
    "\n",
    "    return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, validate_losses):\n",
    "    plt.plot(train_losses, label=\"Training\")\n",
    "    plt.plot(validate_losses, label=\"Validation\")\n",
    "\n",
    "    plt.title(\"Training and Validation Log Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Log Loss\")\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.loss_counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.loss_counter += 1\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.loss_counter += 1\n",
    "            if self.loss_counter >= self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_columnwise_log_loss(y_true, y_pred):\n",
    "    # Clip the predictions to prevent log(0)\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "\n",
    "    # Compute log loss for each label (column)\n",
    "    log_loss_per_column = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred), axis=0)\n",
    "\n",
    "    # Return the mean log loss across all labels\n",
    "    return np.mean(log_loss_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pytorch Mean Columnwise Log Loss\n",
    "def mean_columnwise_log_loss_torch(y_true, y_pred):\n",
    "    # Clip the predictions to prevent log(0)\n",
    "    epsilon = 1e-7\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "\n",
    "    # Compute log loss for each label (column)\n",
    "    log_loss_per_column = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred)).mean(dim=0)\n",
    "\n",
    "    # Return the mean log loss across all labels\n",
    "    return log_loss_per_column.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_labels(train_df):\n",
    "    # dropping the filename from our labels\n",
    "    labels = train_df.drop(columns=[\"filename\"]).to_numpy()\n",
    "    labels = labels.astype(np.float32)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_image_paths(image_set=ImageSet.resized):\n",
    "\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    if ImageSet.resized == image_set:\n",
    "        train_image_paths = train_df[\"filename\"].apply(lambda x: f\"{KAGGLE_PATH}/train_resized_images/{x}.jpg\").tolist()\n",
    "    else:\n",
    "        train_image_paths = train_df[\"filename\"].apply(lambda x: f\"{KAGGLE_PATH}/train_images/{x}.jpg\").tolist()\n",
    "\n",
    "    print(f\"Checking Image Paths:\")\n",
    "    print(f\"{train_image_paths[:10]}\")\n",
    "\n",
    "    return train_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_images_data(image_set=ImageSet.resized):\n",
    "    # test_images_dir = \"./test_images\"\n",
    "\n",
    "    my_pred_df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "    if ImageSet.resized == image_set:\n",
    "        test_image_paths = my_pred_df[\"filename\"].apply(lambda x: f\"{KAGGLE_PATH}/test_resized_images/{x}.jpg\").tolist()\n",
    "    else:\n",
    "        test_image_paths = my_pred_df[\"filename\"].apply(lambda x: f\"{KAGGLE_PATH}/test_images/{x}.jpg\").tolist()\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_images = load_image_dataset(test_image_paths, image_set, run_test_image=True)\n",
    "    end_time = time.time()\n",
    "    print(f\"Test Images Runtime: {end_time - start_time} seconds\")\n",
    "\n",
    "    return my_pred_df, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_images_data(image_set=ImageSet.resized, train_inception=False):\n",
    "    train_image_paths = get_train_image_paths(image_set)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_images = load_image_dataset(\n",
    "        train_image_paths,\n",
    "        image_set,\n",
    "        train_inception\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Training Image Runtime: {end_time - start_time} seconds\")\n",
    "\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    train_image_paths = train_df[\"filename\"].apply(lambda x: f\"{KAGGLE_PATH}/train_resized_images/{x}.jpg\").tolist()\n",
    "\n",
    "    labels = get_image_labels(train_df)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        train_images, labels,\n",
    "        test_size=0.2,\n",
    "        random_state=9\n",
    "    )\n",
    "\n",
    "    ds_train_images, ds_val_images = generate_tensor_datasets(\n",
    "        x_train,\n",
    "        x_val,\n",
    "        y_train,\n",
    "        y_val,\n",
    "    )\n",
    "\n",
    "    return ds_train_images, ds_val_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop Images Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_images_data():\n",
    "    current_dir = os.getcwd()\n",
    "    crop_images_dir = os.path.join(current_dir, \"crop\")\n",
    "\n",
    "    ds_crop = ImageFolder(crop_images_dir, transform=crop_image_transform())\n",
    "\n",
    "    ds_crop_train, ds_crop_val = random_split(\n",
    "        ds_crop,\n",
    "        [0.8, 0.2],\n",
    "        generator=torch.Generator().manual_seed(22)\n",
    "    )\n",
    "\n",
    "    num_classes = len(ds_crop.classes)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    return ds_crop_train, ds_crop_val, ds_crop.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resized Images Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(x_train, x_val, y_train, y_val, test_images):\n",
    "    x_train_tensor = torch.Tensor(x_train)\n",
    "    x_val_tensor = torch.Tensor(x_val)\n",
    "\n",
    "    y_train_tensor = torch.Tensor(y_train)\n",
    "    y_val_tensor = torch.Tensor(y_val)\n",
    "\n",
    "    # Resized Training Images\n",
    "    ds_train_images = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    ds_val_images = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "    # Resized Test Images\n",
    "    test_image_tensor = torch.Tensor(test_images)\n",
    "    ds_test_images = TensorDataset(test_image_tensor)\n",
    "\n",
    "    return ds_train_images, ds_val_images, ds_test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, loss_fn, optimizer, use_cropped):\n",
    "    log_losses = []\n",
    "\n",
    "    model.train()\n",
    "    for batch_index, (x, y) in enumerate(dataloader):\n",
    "        x = x.to(DEVICE)\n",
    "        # if is_crop:\n",
    "        #     y = y.unsqueeze(1).to(DEVICE)\n",
    "        # else:\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        pred_y = model(x)\n",
    "\n",
    "        # this should only trigger on inception nets\n",
    "        # if isinstance(pred_y, tuple):\n",
    "        #     pred_y = pred_y[0]\n",
    "        loss = loss_fn(pred_y, y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_index % 10 == 0:\n",
    "            training_loss = mean_columnwise_log_loss_torch(y, pred_y)\n",
    "            print(f\"Training {training_loss=:.5f}\")\n",
    "            log_losses.append(training_loss.cpu().detach())\n",
    "    return np.mean(log_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(dataloader, model, loss_fn, use_cropped):\n",
    "    log_losses = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            # if is_crop:\n",
    "            #     y = y.unsqueeze(1).to(DEVICE)\n",
    "            # else:\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            pred_y = model(x)\n",
    "            # if isinstance(pred_y, tuple):\n",
    "            #     pred_y = pred_y[0]\n",
    "            # loss = loss_fn(pred_y, y)\n",
    "            validate_loss = mean_columnwise_log_loss_torch(y, pred_y)\n",
    "            log_losses.append(validate_loss.cpu().detach())\n",
    "            print(f\"Validate {validate_loss=:.5f}\")\n",
    "\n",
    "    return np.mean(log_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_test(\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        image_set=ImageSet.resized,\n",
    "        train_on_cropped=False,\n",
    "        train_inception_net=False\n",
    "    ):\n",
    "    if ImageSet.cropped == image_set:\n",
    "        ds_crop_train, ds_crop_validate, _class_list = get_cropped_images_data()\n",
    "        train_dl = DataLoader(ds_crop_train, batch_size=batch_size)\n",
    "        test_dl = DataLoader(ds_crop_validate, batch_size=batch_size)\n",
    "    else:\n",
    "        ds_train_images, ds_val_images = get_train_images_data(image_set, train_inception_net)\n",
    "        train_dl = DataLoader(ds_train_images, batch_size=batch_size)\n",
    "        test_dl = DataLoader(ds_val_images, batch_size=batch_size)\n",
    "\n",
    "    train_log_loss = []\n",
    "    valid_log_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"---- Epoch: {epoch+1} ----\")\n",
    "        train_log_loss.append(train_model(train_dl, model, loss_fn, optimizer, train_on_cropped))\n",
    "        valid_log_loss.append(validate_model(test_dl, model, loss_fn, train_on_cropped))\n",
    "\n",
    "    return train_log_loss, valid_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_submission(nn_model, model_name, image_set, batch_size):\n",
    "    my_pred_df, test_images = get_test_images_data(image_set)\n",
    "\n",
    "    test_images_dl = generate_test_dataloader(test_images, batch_size)\n",
    "\n",
    "    nn_model.eval()\n",
    "    test_predictions = None\n",
    "    with torch.no_grad():\n",
    "        for x in test_images_dl:\n",
    "            x = x[0].to(DEVICE)\n",
    "            pred_y = nn_model(x)\n",
    "            np_y = pred_y.cpu().detach().numpy()\n",
    "            if test_predictions is None:\n",
    "                test_predictions = np_y\n",
    "            else:\n",
    "                test_predictions = np.concatenate((test_predictions, np_y), axis=0)\n",
    "\n",
    "        # test_image_tensor = test_image_tensor.to(DEVICE)\n",
    "        # test_predictions = nn_model(test_image_tensor)\n",
    "        # test_pred_np = test_predictions.cpu().detach().numpy()\n",
    "        my_pred_df.iloc[:, 1:] = test_predictions #test_pred_np\n",
    "\n",
    "    my_pred_df.to_csv(f\"submissions/torch_{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing my own AlexNet with a sigmoid layer\n",
    "class AlexNet5(nn.Module):\n",
    "    def __init__(self, num_classes, linear_in, dropout_1=0.1, dropout_2=0.2):\n",
    "        super(AlexNet5, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(192)\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv_block5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
    "\n",
    "        self.drop_layer1 = nn.Dropout(p=dropout_1)\n",
    "        self.drop_layer2 = nn.Dropout(p=dropout_2)\n",
    "\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(linear_in, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block1(x)\n",
    "        out = self.drop_layer2(out)\n",
    "\n",
    "        out = self.conv_block2(out)\n",
    "        out = self.drop_layer2(out)\n",
    "\n",
    "        out = self.conv_block3(out)\n",
    "\n",
    "        out = self.conv_block4(out)\n",
    "\n",
    "        out = self.conv_block5(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "\n",
    "        out = self.linear_block(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN7(nn.Module):\n",
    "    def __init__(self, num_classes, linear1_in) -> None:\n",
    "        super(CNN7, self).__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(linear1_in, 512)\n",
    "        self.linear_layer2 = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block1(x)\n",
    "        out = self.conv_block2(out)\n",
    "        out = self.conv_block3(out)\n",
    "        out = self.conv_block4(out)\n",
    "\n",
    "        out = torch.flatten(out, 1)\n",
    "\n",
    "        out = self.linear_layer1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear_layer2(out)\n",
    "\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Model & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR = 0.001\n",
    "DECAY_RATE = 0.1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "# we know this to be 15, we can use dataloader on cropped image folder to verify\n",
    "_NUM_CLASSES = 15\n",
    "\n",
    "IMAGE_SET = ImageSet.resized\n",
    "\n",
    "ALEXNET_IMG200_IN = 9216\n",
    "\n",
    "# W * L * 512\n",
    "# 61952 for 200X200\n",
    "# 86528 for 224X224\n",
    "CNN_IMG200_IN = 61952\n",
    "CNN_IMG224_IN = 86528\n",
    "CNN_IMG244_IN = 100352\n",
    "CNN_IMG256_IN = 115200\n",
    "\n",
    "# Model iteration\n",
    "VERSION = \"1\"\n",
    "# nn_model = AlexNet5(num_classes=_NUM_CLASSES, linear_in=ALEXNET_IMG200_IN).to(DEVICE)\n",
    "# nn_model = CNN7(num_classes=_NUM_CLASSES, linear1_in=CNN_IMG200_IN).to(DEVICE)\n",
    "\n",
    "nn_model = models.efficientnet_b4(weights=True, progress=True, num_classes=_NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_losses, validation_log_losses = run_train_test(\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    model=nn_model,\n",
    "    loss_fn=nn.BCELoss(),\n",
    "    optimizer=optim.RMSprop(nn_model.parameters(), lr=LR, weight_decay=DECAY_RATE, momentum=0.8),\n",
    "    image_set=IMAGE_SET,\n",
    "    train_on_cropped=False,\n",
    "    train_inception_net=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(training_log_losses, validation_log_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load Torch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ImageSet.resized == IMAGE_SET:\n",
    "    image_size = 200\n",
    "else:\n",
    "    image_size = IMAGE_RESIZE[0]\n",
    "\n",
    "model_name = f\"{nn_model.__class__.__name__}_v{VERSION}_epoch{EPOCHS}_batch{BATCH_SIZE}_lr{LR}_decay{DECAY_RATE}_resize{image_size}_transform_rotate\"\n",
    "model_path = os.path.join(\"models\", f\"{model_name}.pt\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn_model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_submission(nn_model, model_name, IMAGE_SET, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerva_ai_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
